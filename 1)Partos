import os
import urllib.parse
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text

# ========= CONFIG =========
SERVER   = "DESKTOP-3I6O45D"
DATABASE = "EVALUACION_ESTABLOS"
USER     = "sa"
PASSWORD = "123456789"

BASE_PATH = r"D:\PRUEBA PYTHON"
ESTABLOS = [
    "AGHOLSTEIN","ANGUS","ANTONIOJAIME","BALI","CANELAS","ESTRELLADEMANUEL",
    "GASUR","HUERTOS SAN MARTIN","LAREDO","MOCHERA","PIAMONTE","REMANSO",
    "SANTA FE","SAUSALITO","SEQUION"
]

# === ORDEN EXACTO DE COLUMNAS A INSERTAR (SIN IDENTITY) ===
COLUMNAS_SQL = [
    "Source.Name","DairyName","TODAY","cow","lc","dim","stat","bd","typ",
    "ClvVAIdCode","ClvNumLact","ClvCtrlCode","ClvOffC","ClvCom","ClvCom2",
    "ClvCount","ClvCntLf","ClvCost","ClvRevCode","ClvDim","ClvAge","ClvTech",
    "ClvClvEase","ClvClvEaseCod","ClvDat","ClvTime","Clv2Do","ClvSidEffL2",
    "ClvDiag","Source.FileDate","Source.FileTime"
]

# ========= CONEXI√ìN =========
def _make_engine(driver_name: str, encrypt_yes: bool):
    conn = (
        f"DRIVER={{{driver_name}}};SERVER={SERVER};DATABASE={DATABASE};UID={USER};PWD={PASSWORD};"
        f"TrustServerCertificate=Yes;Encrypt={'Yes' if encrypt_yes else 'No'};"
    )
    return create_engine(
        "mssql+pyodbc:///?odbc_connect=" + urllib.parse.quote_plus(conn),
        fast_executemany=True, pool_pre_ping=True
    )

def get_engine():
    for drv, enc in [("ODBC Driver 18 for SQL Server", True),
                     ("ODBC Driver 17 for SQL Server", False)]:
        try:
            eng = _make_engine(drv, enc)
            with eng.connect():
                pass
            print(f"Conectado con {drv}")
            return eng
        except Exception as e:
            print(f"No se pudo con {drv}: {e}")
    raise RuntimeError("No se pudo abrir conexi√≥n ODBC 17/18.")

ENGINE = get_engine()

# ========= NORMALIZACIONES =========
def to_date_series(s: pd.Series) -> pd.Series:
    return pd.to_datetime(s, errors="coerce").dt.date

def excel_days_to_time_from_series(s: pd.Series) -> pd.Series:
    # Excel almacena tiempos como fracci√≥n de d√≠a; 1899-12-30 base
    td = pd.to_timedelta(pd.to_numeric(s, errors="coerce"), unit="D")
    t = (pd.Timestamp("1899-12-30") + td).dt.time
    return t

def format_time_series_to_string(tser: pd.Series) -> pd.Series:
    # Devuelve 'HH:MM:SS.mmm' o None
    def fmt(x):
        if pd.isna(x):
            return None
        try:
            return f"{x.strftime('%H:%M:%S.%f')[:-3]}"
        except Exception:
            return None
    return tser.apply(fmt)

def normalize_clvtime(col: pd.Series) -> pd.Series:
    s = col.copy()
    if pd.api.types.is_timedelta64_dtype(s):
        t = (pd.Timestamp("1900-01-01") + s).dt.time
        return format_time_series_to_string(t)
    if pd.api.types.is_numeric_dtype(s):
        t = excel_days_to_time_from_series(s)
        return format_time_series_to_string(t)
    # intentar parsear strings y datetimes
    t = pd.to_datetime(s, errors="coerce").dt.time
    return format_time_series_to_string(t)

def normalize_dataframe(df: pd.DataFrame, establo: str) -> pd.DataFrame:
    # Asegura columnas y orden
    for c in COLUMNAS_SQL:
        if c not in df.columns:
            df[c] = np.nan
    df = df[COLUMNAS_SQL]

    # Defaults
    df["DairyName"] = df["DairyName"].fillna(establo)

    # Fechas
    for c in ["TODAY","bd","ClvDat","Clv2Do","Source.FileDate"]:
        df[c] = to_date_series(df[c])

    # Hora como VARCHAR(64)
    df["ClvTime"] = normalize_clvtime(df["ClvTime"])

    # Texto
    varchar_cols = ["Source.Name","DairyName","cow","stat","typ","ClvVAIdCode",
                    "ClvCtrlCode","ClvOffC","ClvCom","ClvCom2","ClvRevCode",
                    "ClvTech","ClvClvEase","ClvClvEaseCod","ClvDiag","ClvSidEffL2",
                    "Source.FileTime","ClvTime"]
    for c in varchar_cols:
        df[c] = df[c].astype(str).where(df[c].notna(), None)

    # cow puede venir num√©rica: convertir a texto sin .0
    df["cow"] = df["cow"].apply(lambda x: None if pd.isna(x) else str(int(x)) if isinstance(x,(int,np.integer)) or (isinstance(x,float) and x.is_integer()) else str(x))

    # N√∫meros
    for c in ["lc","dim","ClvNumLact","ClvCount","ClvCntLf","ClvDim"]:
        df[c] = pd.to_numeric(df[c], errors="coerce").astype("Int64")
    df["ClvCost"] = pd.to_numeric(df["ClvCost"], errors="coerce")
    df["ClvAge"]  = pd.to_numeric(df["ClvAge"],  errors="coerce")

    return df

# ========= UTILIDADES =========
def ts_str(dt: pd.Timestamp) -> str:
    ms3 = dt.microsecond // 1000
    return f"{dt.strftime('%Y-%m-%d %H:%M:%S')}.{ms3:03d}"

# ========= √çNDICE SQL + ESCANEO FS =========
def cargar_indice_existente() -> dict:
    try:
        q = """
        SELECT [Source.Name] AS SName,
               CONVERT(varchar(10), [Source.FileDate], 120) + ' ' + CONVERT(varchar(12), [Source.FileTime]) AS STS
        FROM dbo.PARTOS
        WHERE [Source.Name] IS NOT NULL AND [Source.FileDate] IS NOT NULL AND [Source.FileTime] IS NOT NULL
        GROUP BY [Source.Name], [Source.FileDate], [Source.FileTime]
        """
        df = pd.read_sql_query(q, ENGINE)
        idx = (df.assign(dt=pd.to_datetime(df["STS"], errors="coerce"))
                 .sort_values(["SName","dt"])
                 .dropna(subset=["dt"])
                 .groupby("SName")["STS"].last()
                 .to_dict())
        return idx
    except Exception as e:
        print(f"No se pudo leer √≠ndice existente: {e}")
        return {}

def escanear_archivos_fs() -> set:
    presentes = set()
    for establo in ESTABLOS:
        carpeta = os.path.join(BASE_PATH, establo, "PARTOS")
        if not os.path.isdir(carpeta):
            continue
        for f in os.listdir(carpeta):
            if f.lower().endswith(".xlsx") and not f.startswith("~$"):
                presentes.add(f)
    return presentes

# ========= BORRADO POR ARCHIVOS ELIMINADOS =========
def borrar_faltantes_en_fs():
    indice = cargar_indice_existente()
    en_sql = set(indice.keys())
    en_fs  = escanear_archivos_fs()
    faltantes = sorted(en_sql - en_fs)
    if not faltantes:
        print("‚úî No hay archivos eliminados en FS pendientes de borrar en SQL.")
        return
    print(f"üßπ Borrando {len(faltantes)} archivos inexistentes en FS‚Ä¶")
    with ENGINE.begin() as con:
        for nm in faltantes:
            con.execute(text("DELETE FROM dbo.PARTOS WHERE [Source.Name] = :nm"), {"nm": nm})
            print(f"   - eliminado en SQL: {nm}")

# ========= CARGA INCREMENTAL =========
def cargar_establos_incremental():
    indice = cargar_indice_existente()
    columnas_excel = [c for c in COLUMNAS_SQL if c not in {"Source.Name","Source.FileDate","Source.FileTime"}]

    for establo in ESTABLOS:
        carpeta = os.path.join(BASE_PATH, establo, "PARTOS")
        if not os.path.isdir(carpeta):
            continue

        archivos = [f for f in os.listdir(carpeta) if f.lower().endswith(".xlsx") and not f.startswith("~$")]

        for archivo in archivos:
            ruta = os.path.join(carpeta, archivo)
            try:
                mtime = pd.to_datetime(os.path.getmtime(ruta), unit="s")
                actual_ts = ts_str(mtime)
                ts_prev = indice.get(archivo)

                if ts_prev is not None and pd.to_datetime(actual_ts) <= pd.to_datetime(ts_prev):
                    print(f"‚è© Saltando {archivo} (sin cambios, mtime {actual_ts})")
                    continue

                if ts_prev is not None and pd.to_datetime(actual_ts) > pd.to_datetime(ts_prev):
                    with ENGINE.begin() as con:
                        con.execute(text("DELETE FROM dbo.PARTOS WHERE [Source.Name] = :nm"), {"nm": archivo})
                    print(f"üßπ Eliminado previo de {archivo} (TS previo {ts_prev})")

                # Leer Excel sin encabezado. Los nombres reales los imponemos con columnas_excel
                df = pd.read_excel(ruta, header=None, names=columnas_excel, engine="openpyxl")
                df.dropna(how="all", inplace=True)
                if df.empty:
                    print(f"‚ö† {archivo} vac√≠o, omitido.")
                    continue

                # Metadatos de origen
                df["Source.Name"]     = archivo
                df["Source.FileDate"] = mtime.date()
                df["Source.FileTime"] = actual_ts.split(" ")[1]

                # Normalizar y completar
                df = normalize_dataframe(df, establo)

                print(f"‚è≥ Cargando {archivo} desde {establo} ({actual_ts})‚Ä¶")
                df.to_sql("PARTOS", ENGINE, if_exists="append", index=False, chunksize=1000, schema="dbo")
                print(f"‚úÖ Cargado {archivo}.\n")

                indice[archivo] = actual_ts

            except Exception as e:
                print(f"‚ùå Error al cargar {archivo} desde {establo}: {e}\n")

if __name__ == "__main__":
    borrar_faltantes_en_fs()
    cargar_establos_incremental()
